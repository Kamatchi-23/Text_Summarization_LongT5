# Text_Summarization_LongT5
This is an experimentation project attempted to explore the text summarization task and reproduce the results obtained in the original work by Guo, M and the team from Google that developed the Long T5 base model with the Tglobal attention mechanism. Considering the limitations of the computation resources and time, the evaluation was constrained to only 1000 test records of the PubMed Dataset, which took about 3 hours to obtain the results using T4 GPU. 
Student Name: Kamatchi Gnanavel
Student ID: 24998902

#References
Guo, M., Ainslie, J., Uthus, D. C., Ontañón, S., Ni, J., Sung, Y.-H., & Yang, Y. (2022). LongT5: Efficient Text-To-Text Transformer for Long Sequences. https://doi.org/10.18653/v1/2022.findings-naacl.55 
Summarization. (n.d.). Huggingface.co. https://huggingface.co/docs/transformers/en/tasks/summarization 
Synced. (2021, December 21). Google’s Transformer-Based LongT5 Achieves Performance Gains by Scaling Both Input Length and Model Size. Medium; SyncedReview. https://medium.com/syncedreview/googles-transformer-based-longt5-achieves-performance-gains-by-scaling-both-input-length-and-model-687afb8a3274

